{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c748d99a-875b-40c0-9f15-688452b1cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed!\n",
      "Total RGB images saved: 200\n",
      "Total depth maps saved: 200\n",
      "Total labels saved: 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "from datasets import load_dataset\n",
    "\n",
    "num_samples = 200\n",
    "mat_data = scipy.io.loadmat(\"classMapping40.mat\")\n",
    "old_map = mat_data[\"mapClass\"].squeeze()  \n",
    "old_map = old_map - 1 \n",
    "padded_map = np.zeros(895, dtype=old_map.dtype)\n",
    "padded_map[:894] = old_map\n",
    "padded_map[894] = 39\n",
    "class_mapping = padded_map\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"0jl/NYUv2\",\n",
    "    trust_remote_code=True,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "# Seleziona num_samples indici casuali e distinti nell'intervallo da 0 a 1448\n",
    "indices = np.random.choice(range(1448), size=num_samples, replace=False)\n",
    "subset = dataset.select(indices)\n",
    "\n",
    "os.makedirs(\"rgb_images\", exist_ok=True)\n",
    "os.makedirs(\"depth_maps\", exist_ok=True)\n",
    "os.makedirs(\"labels\", exist_ok=True)\n",
    "\n",
    "rgb_count = 0\n",
    "depth_count = 0\n",
    "label_count = 0\n",
    "\n",
    "for idx, example in enumerate(subset):\n",
    "    try:\n",
    "        image_pil = example[\"image\"]  \n",
    "        image_pil.save(f\"rgb_images/rgb_{idx}.png\")\n",
    "        rgb_count += 1\n",
    "\n",
    "        depth_map = example[\"depth\"]  \n",
    "        depth_norm = (depth_map / np.max(depth_map) * 255).astype(np.uint8)\n",
    "        depth_image = Image.fromarray(depth_norm)\n",
    "        depth_image.save(f\"depth_maps/depth_{idx}.png\")\n",
    "        depth_count += 1\n",
    "\n",
    "        label_map = example[\"label\"] \n",
    "        label_map_40 = class_mapping[label_map] \n",
    "        np.save(f\"labels/label_{idx}.npy\", label_map_40)\n",
    "        label_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore al salvataggio del dato idx={idx}: {e}\")\n",
    "\n",
    "print(\"Data extraction completed!\")\n",
    "print(f\"Total RGB images saved: {rgb_count}\")\n",
    "print(f\"Total depth maps saved: {depth_count}\")\n",
    "print(f\"Total labels saved: {label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403bd953-e04f-4000-8b93-4c40b25ac609",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Creazione del dataset e del dataloader\u001b[39;00m\n\u001b[1;32m    181\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SegmentationDataset(rgb_dir, labels_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m--> 182\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Creazione del modello, definizione dell'ottimizzatore e della loss\u001b[39;00m\n\u001b[1;32m    185\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Definizione della U-Net\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo che esegue due convoluzioni consecutive con kernel 3, ciascuna seguita da\n",
    "    Batch Normalization e attivazione ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo per la fase di downsampling: max pooling seguito da doppia convoluzione.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Modulo per l'upsampling. Può utilizzare l'interpolazione bilineare o la convoluzione trasposta.\n",
    "    Dopo l'upsampling il modulo concatena la feature map proveniente dalla fase di encoder (skip connection)\n",
    "    e applica una doppia convoluzione.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)\n",
    "            )\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            \n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Aggiustamento dimensioni per la concatenazione\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Strato finale che riduce il numero di canali all’output desiderato (40 classi).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementazione della U-Net con fase di encoder, decoder e relative skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Definizione del Dataset\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset per la semantic segmentation che carica le immagini RGB da file PNG e\n",
    "    le relative label da file .npy. Si assume che il nome delle immagini e delle label\n",
    "    condivida lo stesso suffisso numerico (ad esempio: rgb_1.png e labels_1.npy).\n",
    "    \"\"\"\n",
    "    def __init__(self, rgb_dir, labels_dir, transform=None, target_transform=None):\n",
    "        super(SegmentationDataset, self).__init__()\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        # Ordinamento dei file in modo che corrispondano\n",
    "        self.rgb_files = sorted(glob.glob(os.path.join(rgb_dir, \"rgb_*.png\")))\n",
    "        self.label_files = sorted(glob.glob(os.path.join(labels_dir, \"labels_*.npy\")))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        if len(self.rgb_files) != len(self.label_files):\n",
    "            raise ValueError(\"Il numero di immagini RGB e di label non coincide.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rgb_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Caricamento immagine RGB\n",
    "        rgb_path = self.rgb_files[idx]\n",
    "        image = Image.open(rgb_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Caricamento label (file .npy)\n",
    "        label_path = self.label_files[idx]\n",
    "        label = np.load(label_path)\n",
    "        # Convertiamo la label in tensore Long (necessario per la CrossEntropyLoss)\n",
    "        label = torch.from_numpy(label).long()\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Impostazioni: percorso delle cartelle e parametri di training\n",
    "    rgb_dir = os.path.join(\"data\", \"rgb_images\")\n",
    "    labels_dir = os.path.join(\"data\", \"labels\")\n",
    "    num_classes = 40\n",
    "    batch_size = 2\n",
    "    num_epochs = 1  # Per esempio, una sola epoca per testare il funzionamento\n",
    "    \n",
    "    # Definizione delle trasformazioni:\n",
    "    # Le immagini nativamente 480x640 vengono ridimensionate a 320x240 (240,320 in HxW)\n",
    "    # e successivamente center-cropped a 304x228 (228,304 in HxW)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((240, 320)),\n",
    "        transforms.CenterCrop((228, 304)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Creazione del dataset e del dataloader\n",
    "    dataset = SegmentationDataset(rgb_dir, labels_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Creazione del modello, definizione dell'ottimizzatore e della loss\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet(n_channels=3, n_classes=num_classes, bilinear=True).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss si aspetta label in [0, num_classes-1]\n",
    "    \n",
    "    # Ciclo di training di base\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}] terminata. Loss media: {running_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    print(\"Training completato.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
